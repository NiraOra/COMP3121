{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 - Week 1\n",
    "\n",
    "## General Stuff\n",
    "\n",
    "- required knowledge: understanding of data structures and algorithms (arrays, trees, heaps, etc)\n",
    "- review booklet on moodle\n",
    "- not requried to know programming languages; not even pseudocode\n",
    "- SmartThinking (for writing help)\n",
    "- pre-req: COMP2521\n",
    "- demonstrate problem solving and reasoning\n",
    "- textbook: Kleinberg and Tardosh: Algorithm Design and Cormen; Leiserson, Rivest and Stein: Introduction to Algorithms \n",
    "\n",
    "### Assessments\n",
    "\n",
    "- take home quiz: recommended attempt before second tutorial to get feedback\n",
    "  - have to submit by week 4 first\n",
    "- assignments: three questions each; weighted 15% each\n",
    "- test: \n",
    "  - in person and by inspera\n",
    "  - hurdle: atleast 50% in atleast 1 algorithm design problem to pass the course\n",
    "\n",
    "\n",
    "## Algorithms \n",
    "\n",
    "### Overview \n",
    "\n",
    "- course about designing algorithm\n",
    "- What is algorithm ? and how to design these\n",
    "- only dealing with sequential algorithms and deterministic algorithms \n",
    "- emphasis on algorithm designing skills\n",
    "\n",
    "### Question: Two Thieves\n",
    "\n",
    "Question: Alice and Bob have robbed a warehouse and have to split a pile of divisible items. There is no objective valuation of the items (e.g. price tags); instead, Alice and Bob each have a valuation in mind, and their valuations might be different.\n",
    "Design an algorithm to split the pile so that each thief values their own pile as at least half the loot.\n",
    "\n",
    "Solution: You Cut I Choose: Alice splits the pile in two parts, so that she believes that both parts are of equal value. Bob then chooses the part that he believes is no worse than the other.\n",
    "\n",
    "- if only one item and both want it, then only one person can get it and the other will be unhappy\n",
    "\n",
    "Question: Can Alice efficiently split the loot of n items into two equal piles, if this is possible?\n",
    "\n",
    "(A) The fastest known algorithm for this needs exponential time, O(n · 2^(n/2)) ⊆ O(1.4145n).\n",
    "\n",
    "- but if n is large then it is non-trivial\n",
    "- subliminal option in polynomial time ?\n",
    "\n",
    "#### WEAKNESS PROPERTY\n",
    "\n",
    "Proportionality up to one item: each thieve’s share is worth at least half the loot with the most valuable item removed.\n",
    " \n",
    "#### ALGO\n",
    "\n",
    "The thieves alternatively pick their most valuable item, starting with Alice.\n",
    "\n",
    "#### CORRECTNESS\n",
    "\n",
    "Divide the picking sequence into rounds os size 2.\n",
    "Alice always picks a more valuable item than Bob in each round. So, she receives at least half her value of the loot.\n",
    "In each round, Bob picks a more valuable item than Alice picks in the next round. So, if we remove Alice’s first picked item from the loot, Bob gets at least half the value.\n",
    "\n",
    "### Question: Three Thieves\n",
    "\n",
    "<b>Question</b>: Alice, Bob and Carol have robbed a warehouse and have to split a pile of divisible items. Each thief has their own valuation of the items.\n",
    "Design an algorithm to split the pile so that each thief values their own pile as at least one third of the loot.\n",
    "\n",
    "- thinking: if Alice splits up the loot into what she thinks is thirds (and they are of different piles), and each of them take an individual pile, then each thief thinks that they got the exact share\n",
    "- if bob and carol take the same pile, then Alice can choose one pile from the other two and the final pile is split between bob and carol; but this doesn't work!\n",
    "\n",
    "Example: if loot split into X, Y, Z, where:\n",
    "- Bob thinks: X = 50%, Y = 40%, Z = 10%\n",
    "- Carol thinks: X = 50%, Y = 10%, Z = 40%.\n",
    "\n",
    "if Bob + Carol choose X, then if Alice chooses:\n",
    "- Y then Bob complains (in his eyes) that 60% remains and it is not fair\n",
    "- Z then Carol complains (in her eyes) that 60% remains and it is not fair\n",
    "- failed because we need to ensure both Bob and Carol think there is 2/3rds is left after they pick the loot\n",
    "\n",
    "<b>Algorithm</b>: \n",
    "\n",
    "1. Alice makes a pile X which she believes is 1/3rds of loot\n",
    "2. Then, she asks Bob if X $\\leq \\frac{1}{3}.$ \n",
    "   1. If Bob says yes, then he'll be happy to share the rest $\\geq \\frac{2}{3}$ with the other thief \n",
    "      1. Alice can then ask Carol if she thinks X $\\leq \\frac{1}{3}.$ \n",
    "         1. If Carol says no, then Carol takes X and Alice and Bob split up the rest of the loot\n",
    "         2. If Carol says yes, then Alice keeps X and Carol and Bob split up the rest of the loot\n",
    "   2. If Bob says No, Alice gives the pile to Bob and asks him to split up till it reaches to a third (in Bob's eyes) and then she get X $< \\frac{1}{3}$, which makes her happy to share the rest with Carol since it is $X > \\frac{2}{3}$\n",
    "      1. This is just Bob in Alice's place in this case. So Bob then asks Carol if she views X $\\leq \\frac{1}{3}.$ \n",
    "         1. If Carol says no, then Carol takes X and Alice and Bob split up the rest of the loot\n",
    "         2. If Carol says yes, then Bob keeps X and Carol and Alice split up the rest of the loot\n",
    "\n",
    "### Question: n thieves\n",
    "\n",
    "- check for nested recursions\n",
    "\n",
    "## Proofs\n",
    "\n",
    "- Mathematical proofs are used to justify things which are not obvious to common sense (to justify algorithms which are not obvious)\n",
    "\n",
    "Example: Merge sort\n",
    "\n",
    "- recursive \n",
    "- lets take subarray A[l...r]\n",
    "- if l = r, nothing to do (base case)\n",
    "- otherwise:\n",
    "  - m = (l + r)/2\n",
    "  - apply algorithm to left A[l...m] and right A[m+1...r] and sort\n",
    "  - then merge A[l...m] and A[m+1...r]\n",
    "  - running time: O(n log_2 n)\n",
    "\n",
    "### Examples\n",
    "\n",
    "- Suppose there are n hospitals in the state, and n new doctors have graduated from university. Each hospital wants to employ exactly one new doctor.\n",
    "- Every hospital submits a list of preferences, which ranks all the doctors, and every doctor submits a list of preferences, which ranks all the hospitals.\n",
    "- We’d like to assign all n doctors to different hospitals in order to “make everyone happy” in some sense.\n",
    "\n",
    "### Stable Matching Problem \n",
    "\n",
    "- A matching is an assignment of doctors to hospitals so that no doctor has more than one job and no hospital has more than one employee.\n",
    "- A perfect matching is a matching involving all doctors and all hospitals.\n",
    "\n",
    "Our task is to design a perfect matching that somehow satisfies the hospitals and doctors, with regards to their preferences.\n",
    "\n",
    "Q: Can we assign every doctor and every hospital their first preference? \n",
    "- not exaclty !! \n",
    "\n",
    "<b> Definition</b>: A stable matching is a perfect matching in which there are no two pairs (h, d) and (h′, d′) such that:\n",
    "\n",
    "- hospital h prefers doctor d′ to doctor d, and \n",
    "- doctor d′ prefers hospital h to hospital h′.\n",
    "\n",
    "[example in slide 44 of introduction slides]\n",
    "\n",
    "### n = 2\n",
    "Another example: Up to symmetry, there are two more cases to consider. Solve them.\n",
    "\n",
    "The following rules cover all situations with two hospitals and two doctors.\n",
    "- If the hospitals prefer different doctors, assign each hospital their preferred doctor. Neither hospital wants to swap, so the matching is stable.\n",
    "- The same applies vice versa, i.e. if the doctors prefer different hospitals.\n",
    "- However, if both hospitals prefer the same doctor d and both doctors prefer the same hospital h, pair h with d and the other hospital with the other doctor. Neither h nor d wants to swap, so the matching is stable.\n",
    "\n",
    "### Gale - Shapey algorithm\n",
    "\n",
    "Q. Given n hospitals and n doctors, how many ways are there to match them, without regard for preferences?\n",
    "- Yes: n! ≈ $(\\frac{n}{e})^n$ – exponentially many in n (e ≈ 2.71).\n",
    "Q. Is it true that for every possible collection of n lists of preferences provided by all hospitals, and n lists of preferences provided by all doctors, a stable matching always exists?\n",
    "- YES, but this is NOT obvious!\n",
    "\n",
    "\n",
    "Q. Can we find a stable matching in a reasonable amount of time?\n",
    "- YES, using the Gale - Shapley algorithm.\n",
    "\n",
    "It: \n",
    "- Produces pairs in stages, with possible revisions\n",
    "- A hospital which is not currently employing a doctor will be called vacant.\n",
    "- Hospitals will be offering jobs to doctors. Doctors will decide whether they accept a job offer or not. Start with all hospitals vacant.\n",
    "- While there is a vacant hospital which has not offered jobs to all doctors, pick any such vacant hospital and call it h.\n",
    "- Hospital h offers a job to the highest ranking doctor d on its list, ignoring any doctors to whom it has already offered a job.\n",
    "- If d is not yet employed, she accepts the job (at least tentatively).\n",
    "- Otherwise, if d is already employed at a different hospital h′: if d prefers the new hospital h to her current hospital h′, she quits h′ and joins h (making h′ vacant)otherwise, since d prefers her current hospital h′, she rejects the offer from h and stays at h′."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 \n",
    "\n",
    "## Continuing Hospital Question\n",
    "\n",
    "- The algorithm terminates after ≤ n2 rounds.\n",
    "\n",
    "1. Claim 1: In every round of the algorithm, one hospital offers a job to one doctor.\n",
    "\n",
    "Proof:\n",
    "\n",
    "- Every hospital can make an offer to a doctor at most once. Thus, every hospital can make at most n offers.\n",
    "- There are n hospitals, so in total they can make $\\leq n^2$ offers. Thus there can be no more than n2 rounds.\n",
    "\n",
    "2. Claim 2: The algorithm produces a perfect matching, i.e., every hospital is eventually paired with a doctor (and thus also every doctor is paired to a hospital).\n",
    "\n",
    "Proof\n",
    "\n",
    "- Assume that the algorithm has terminated, but hospital h is still vacant.\n",
    "- This means that h has already offered a job to every doctor. A doctor is unemployed only if no hospital has offered them a job, so all doctors must have a job.\n",
    "- But this would mean that n doctors are paired with all of n hospitals, so h cannot be vacant.\n",
    "- This is a contradiction, completing the proof.\n",
    "\n",
    "3. Claim 3: The matching produced by the algorithm is stable.\n",
    "\n",
    "- Recall that each hospital makes offers to doctors in order from its most preferred to its least preferred.\n",
    "- Therefore, the sequence of doctors employed at a particular hospital is in this order also.\n",
    "- Each doctor is initially unemployed, then takes the first offer made to them, and only ever moves to a hospital they prefer to their current employer.\n",
    "- Thus, each doctor is paired with hospitals in order from their least preferred to their most preferred.\n",
    "\n",
    "Proof: (using proof of contradiction)\n",
    "\n",
    "- We will prove that the matching is stable using proof by contradiction.\n",
    "- Assume that the matching is not stable. Thus, there are two pairs (h, d) and (h′, d′) such that:\n",
    "    - h prefers d′ over d and \n",
    "    - d′ prefers h over h′.\n",
    "- Since h prefers d′ over d, it must have made an offer to d′ before offering the job to d.\n",
    "- Since h is paired with d, doctor d′ must have either:\n",
    "  - rejected h because they were already at a hospital they prefer to h, or\n",
    "  - accepted h only to later rescind this and accept an offer from a hospital they prefer to h.\n",
    "- In both cases d′ would now be at a hospital which they prefer over h.\n",
    "- This is a contradiction, completing the proof.\n",
    "\n",
    "## Puzzles\n",
    "\n",
    "Problem:\n",
    "\n",
    "Q. Tom and his wife Mary went to a party where nine more couples were present.\n",
    "- Not every one knew everyone else, so people who did not know each other introduced themselves and shook hands.\n",
    "- People who knew each other from before did not shake hands.\n",
    "- Later that evening Tom got bored, so he walked around and asked all other guests (including his wife) how many hands they had shaken that evening, and got 19 different answers.\n",
    "- How many hands did Mary shake? \n",
    "- How many hands did Tom shake?\n",
    "\n",
    "## Complexity\n",
    "\n",
    "### Fast and Slow Algorithms\n",
    "\n",
    "- You should be familiar with true-ish statements such as: \"Heap sort is faster than bubble sort. Linear search is slower than binary search.\"\n",
    "- We would like to make such statements more precise. \n",
    "- We also want to understand when they are wrong, and why it matters.\n",
    "\n",
    "### Best and worst case performance\n",
    "\n",
    "- The statements on the previous slide are most commonly made in comparing the worst case performance of these algorithms.\n",
    "- They are actually false in the best case!\n",
    "- In most problems in this course, we are concerned with worst case performance, so as to be robust to maliciously created (or simply unlucky) instances.\n",
    "- We will hardly ever discuss the best case.\n",
    "\n",
    "### Average case performance\n",
    "\n",
    "- In some circumstances, one might accept occasional poor performance as a trade-off for good average performance over all possible inputs (or a large sample of them).\n",
    "- Analysing the average case or expected performance of an algorithm requires probabilistic methods and is beyond the scope of this course, except where we rely purely on results of this type from prior courses (e.g. hash table operations, quicksort).\n",
    "\n",
    "### Rates of Growth\n",
    "\n",
    "- We need a way to compare two functions, in this case representing the worst case running time of each algorithm.\n",
    "- A natural starting point is to compare function values directly.\n",
    "Q.  If f (100) > g (100), then does f represent a greater running time, i.e. a slower algorithm?\n",
    "A. Not necessarily! n = 100 might be an outlier, or too small to appreciate the efficiencies of algorithm f . We care more about which algorithm scales better.\n",
    "\n",
    "- We prefer to talk in terms of asymptotics, i.e. long-run behaviour.\n",
    "  - For example, if the size of the input doubles, the function value could (approximately) double, quadruple, etc.\n",
    "  - A function which quadruples will eventually exceed a function which doubles, regardless of the values for small inputs.\n",
    "  - We’d like to categorise functions (i.e. running times, and therefore algorithms) by their asymptotic rate of growth.\n",
    "\n",
    "### Big Oh notation\n",
    "\n",
    "Definition: We say f (n) = O(g(n)) if for large enough n, f (n) is at most a constant multiple of g(n).\n",
    "\n",
    "> Large enough ? how large: Everything is small compared to the infinity of values beyond a particular value for n. It doesn’t matter how f (1) compares to g (1), or even how f (1, 000, 000) compares to g (1, 000, 000). We only care that f (n) is bounded above by a multiple of g(n) eventually.\n",
    "> <break>\n",
    "> When choosing algorithms for a particular application with inputs of size at most 1, 000, 000, the behaviour beyond this size doesn’t matter at al\n",
    "\n",
    "- g (n) is said to be an asymptotic upper bound for f (n).\n",
    "- This means that the rate of growth of function f is no greater than that of function g.\n",
    "- An algorithm whose running time is f (n) scales at least as well as one whose running time is g(n).\n",
    "- It’s true-ish to say that the former algorithm is ‘at least as fast as’ the latter.\n",
    "- Useful to (over-)estimate the complexity of a particular algorithm.\n",
    "- For uncomplicated functions, such as those that typically arise as the running time of an algorithm, we usually only have to consider the dominant term.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "1. Let f (n) = 100n. Then f (n) = O(n), because f (n) is at most 100 times n for large n.\n",
    "2. Let f(n) = 2n + 7. Thenf(n)=O(n2),becausef(n)isatmost1 times n2 for large n. Note that f (n) = O(n) is also true.\n",
    "3. Let f (n) = 0.001n3. Then f (n) ̸= O(n2), because for any constant multiple of n2, f (n) will eventually exceed it.\n",
    "\n",
    "### Big Omega Notation\n",
    "\n",
    "Definition: We say f (n) = Ω(g(n)) if for large enough n, f (n) is at least a constant multiple of g(n).\n",
    "\n",
    "- g (n) is said to be an asymptotic lower bound for f (n). This means that the rate of growth of function f is no less than that of function g.\n",
    "- An algorithm whose running time is f (n) scales at least as badly as one whose running time is g(n).\n",
    "- It’s true-ish to say that the former algorithm is ‘no faster than’ the latter.\n",
    "- Useful to say that any algorithm solving a particular problem runs in at least Ω(g(n)).\n",
    "- For example, finding the maximum element of an unsorted array takes Ω(n) time, because you must consider every element.\n",
    "- Once again, for every function that we care about, only the dominant term will be relevant.\n",
    "\n",
    "> a proof question: Using these formal definitions, prove that if f (n) = O(g(n)) then g (n) = Ω(f (n)).\n",
    "\n",
    "### Big Theta Notation\n",
    "\n",
    "Definition: We say f (n) = Θ(g(n)) if f (n) = O(g(n)) and f (n) = Ω(g(n)). (f(n) and g(n) have same asymptotic growth)\n",
    "\n",
    "- It’s true-ish to say that the former algorithm is ‘as fast as’ the latter.\n",
    "\n",
    "> Statements such as \"bubble sort runs in O(n2) time in the worst case.\" exist. Should these statements be written using Θ(·) instead of O(·)? \n",
    "> <break>\n",
    "> They can, but they don’t have to be. The statements bubble sort runs in O(n2) time in the worst case and bubble sort runs in Θ(n2) time in the worst case are both true: they claim that the worst case running time is at most quadratic and exactly quadratic respectively. (but theta statements convey slightly more than O; however it is not necessary in most cases)\n",
    "> TLDR: O and $\\Theta$ can be used interchangebly\n",
    "\n",
    "### Properties\n",
    "\n",
    "1. Sum Property: If f1 = O(g1) and f2 = O(g2), then f1+f2 = O(g1+g2).\n",
    "   1. last term O(g1 + g2) is often rewritten as O (max(g1, g2)), since g1 + g2 ≤ 2 max(g1, g2). (same for $\\Theta$ or $\\Omega$)\n",
    "   2.  justifies ignoring non-dominant terms: if f2 has a lower asymptotic bound than f1, then the bound on f1 also applies to f1 + f2. (eg: if f2 is linear but f1 is quadratic, then f1 + f2 is also quadratic.)\n",
    "   3.  useful for analysing algorithms that have two or more stages executed sequentially.\n",
    "   4.  just take the most \"expensive\" stage of the two (if f1 = O(n) and f2 is O($n^2$) then take $O(n^2)$)\n",
    "2. Product Property: If f1 = O(g1) and f2 = O(g2),then f1·f2 = O(g1·g2).\n",
    "   1. In particular, if f = O(g) and λ is a constant (i.e. λ = O(1)), thenλ·f =O(g)also.\n",
    "   2. The same property applies if O is replaced by Ω or Θ. \n",
    "   3. This is useful for analysing algorithms that have two or more nested parts.\n",
    "   4. If each execution of the inner part takes f2 time, and it is executed f1 many times, then we can bound each part and multiply the bounds.\n",
    "\n",
    "## Logarithms\n",
    "\n",
    "Definition: For $a,b > 0 \\text{ and } a \\not= 1,\\text{ let } n = loga_b \\text{ if } a^n =b.$\n",
    "\n",
    "- same log properties apply\n",
    "\n",
    "### Change of base rule\n",
    "\n",
    "Definition: $log_a(x) = \\frac{log_b(x)}{log_b(a)}$\n",
    "\n",
    "- The denominator is constant with respect to x!\n",
    "- Therefore loga n = Θ(logb n), that is, logarithms of any base are interchangeable in asymptotic notation.\n",
    "- We typically write log n instead, suppressing the base.\n",
    "\n",
    "## Data Structures\n",
    "\n",
    "### Hash Tables\n",
    "\n",
    "- Store values indexed by keys.\n",
    "- Hash function maps keys to indices in a fixed size table.\n",
    "- Ideally no two keys map to the same index, but this is not guaranteed.\n",
    "- A situation where two (or more) keys have the same hash value is called a collision.\n",
    "- There are several ways to resolve collisions – for example, the separate chaining method stores a linked list of all colliding key-value pairs at each index of the hash table.\n",
    "\n",
    "Operations (expected)\n",
    "- Search for the value associated to a given key: O(1) Update the value associated to a given key: O(1) Insert/delete: O(1)\n",
    "\n",
    "Operations (worst case)\n",
    "- Search for the value associated to a given key: O(n) Update the value associated to a given key: O(n) Insert/delete: O(n)\n",
    "\n",
    "### Binary Search Trees\n",
    "\n",
    "- Store (comparable) keys or key-value pairs in a binary tree, where each node has at most two children, designated as left and right\n",
    "- Each node’s key is greater than all keys in its left subtree, and less than all keys in its right subtree.\n",
    "\n",
    "Operations\n",
    "- Let h be the height of the tree, that is, the length of the longest path from the root to the leaf.\n",
    "  - Search: O(h) \n",
    "  - Insert/delete: O(h)\n",
    "\n",
    "#### Self Balancing Binary Search Trees\n",
    "\n",
    "- In the best case, h ≈ log2 n. Such trees are said to be balanced.\n",
    "- In the worst case, h ≈ n\n",
    "- Fortunately, there are several ways to make a self-balancing binary search tree (e.g., AVL tree, red-black tree).\n",
    "- Each of these performs rotations to maintain certain invariants, in order to guarantee that h = O(log n) and therefore all tree operations run in O(logn).\n",
    "- Red-black trees are detailed in CLRS, but in this course it is sufficient to write “self-balancing binary search tree” without specifying any particular scheme.\n",
    "\n",
    "### Binary Heaps\n",
    "\n",
    "- Store items in a complete binary tree, with every parent comparing ≥ all its children.\n",
    "- This is a max heap; replace ≥ with ≤ for min heap. Used to implement priority queue.\n",
    "\n",
    "Operations\n",
    "\n",
    "- Build heap: O(n)\n",
    "- Find maximum: O(1) \n",
    "- Delete maximum: O(log n) \n",
    "- Insert: O(log n)\n",
    "\n",
    "## Binary Search\n",
    "\n",
    "### Searching a sorted array\n",
    "\n",
    "Problem: You are given a sorted array A of n integers. Determine whether a value x appears in the array.\n",
    "Observation: If A[i] < x, then for all j < i, we have that A[j] < x.\n",
    "\n",
    "### Binary Search \n",
    "\n",
    "Algorithm\n",
    "\n",
    "We recursively apply the following algorithm to the subarray A[l..r] in order to search for x.\n",
    "\n",
    "If the array has no elements (i.e. r < l), then x cannot be found. \n",
    "Otherwise:\n",
    "- first define m = $|\\frac{l + r}{2}|$ (floor), the midpoint of the subarray,\n",
    "- then, we do a case analysis on the value at the midpoint, namely A[m].\n",
    "  - If A[m] = x, we have found an occurrence of x and we can exit the recursion.\n",
    "  - If A[m] > x, we recurse on the left subarray A[l..m − 1].\n",
    "  - If A[m] < x, we recurse on the right subarray A[m + 1..r].\n",
    " \n",
    "Complexity\n",
    "Worst case: O(log n). At each step, the search space halves, which can only happen log2 n times.\n",
    "\n",
    "Small modifications allow us to solve related search problems:\n",
    "- Find the smallest index i such that A[i] ≥ x, etc.\n",
    "- Find the range of indices l..r such that A[l] = ... = A[r] = x.\n",
    "\n",
    "### Decision problems and optimisation problems\n",
    "\n",
    "- Consider decision problems of the form: Given some input including x, can you ...\n",
    "- and optimisation problems of the form: What is the smallest x for which you can . . .\n",
    "- An optimisation problem is typically harder than the corresponding decision problem.\n",
    "- Can we solve (some) optimisation problems quickly by reducing them to decision problems?\n",
    "\n",
    "### Discrete Binary Search \n",
    "\n",
    "- Assume that x is an integer. Similar ideas work for real x. \n",
    "- Let f (x) = 0 if the decision problem returns NO; 1 otherwise\n",
    "- In some (but not all) such problems, if the condition holds for x then it also holds for x + 1. In that case, we say that the Monotonicity property holds: f (x) ≤ f (x + 1)\n",
    "- Thus f is all 0’s up to the first 1, after which it is all 1’s. So we can use binary search to find the smallest x with f (x ) = 1.\n",
    "- This technique of binary searching the answer, that is, finding the smallest x such that f (x ) = 1 using binary search, is often called discrete binary search.\n",
    "- Overhead is just a factor of O (log |A|) where A is the range of possible answers.\n",
    "\n",
    "## Sorting \n",
    "\n",
    "### Comparison Sorting\n",
    "\n",
    "Problem: You are given an array A consisting of n items. The following operations each take constant time:\n",
    "- read from any index \n",
    "- write to any index \n",
    "- compare two items.\n",
    "\n",
    "- Design an efficient algorithm to sort the items.\n",
    "- Bubble sort, selection sort and insertion sort all take O($n^2$) time in the worst case.\n",
    "\n",
    "### Merge Sort\n",
    "\n",
    "Algorithm\n",
    "1. If n = 1, do nothing. Otherwise, let m = ⌈n/2⌉.\n",
    "2. Apply merge sort recursively to A[1..m] and A[m + 1..n].\n",
    "3. Merge A[1..m] and A[m + 1..n] into A[1..n].\n",
    "\n",
    "Complexity: Best case: O(n log n). Worst case: O(n log n). Space: O(n).\n",
    "\n",
    "Conclusions\n",
    "- Reliably fast for large arrays. \n",
    "- Space requirement is a drawback. \n",
    "- Useful in some circumstances:\n",
    "  - if a stable sort is required \n",
    "  - when sorting a linked list \n",
    "  - with parallelisation.\n",
    "\n",
    "### Heapsort\n",
    "\n",
    "Algorithm:\n",
    "1. Construct a min heap from the elements of A.\n",
    "2. Write the top element of the heap to A[1] and pop it from the heap.\n",
    "3. Repeat the previous step until the heap is empty.\n",
    "\n",
    "Correctness: Obvious, since the top element of the heap is always the smallest remaining.\n",
    "\n",
    "Complexity\n",
    "- Best case: O(n log n).\n",
    "- Worst case: O(n log n).\n",
    "- Selection sort but with selection in O(log n) rather than O(n).\n",
    "\n",
    "\n",
    "Conclusions\n",
    "- Reliably fast for large arrays. \n",
    "- No additional space required.\n",
    "- Constant factor is larger than other fast sorts, so used less in practice.\n",
    "\n",
    "### Quicksort\n",
    "\n",
    "Algorithm\n",
    "1. Designate the first element as the pivot.\n",
    "2. Rearrange the array so that all smaller elements are to the left of the pivot, and all larger elements to its right.\n",
    "3. Recurse on the subarrays left and right of the pivot.\n",
    "\n",
    "Correctness: Obvious.\n",
    "\n",
    "Complexity\n",
    "- The second step (rearranging and partitioning the array) can be done in O(n) without using additional memory (how?).\n",
    "- However, the performance still greatly depends on the pivot used.\n",
    "  - In the best case, the pivot is always the median, so the subarrays each have size about n/2; like mergesort, this is O(n log n).\n",
    "  - In the worst case, the pivot is always the minimum (or maximum), so one subarray is empty and the other has size n − 1; like selection sort, this is O($n^2$).\n",
    "- Fortunately, the worst case is rare.\n",
    "- The average case runtime is O(n log n).\n",
    "- Any element could be the pivot! Better pivot selection strategies include:\n",
    "  - select an array element at random\n",
    "  - ‘median-of-three’: among the first, middle and last elements, select the median\n",
    "  - ‘median-of-medians’: more on this next week.\n",
    "\n",
    "Conclusions\n",
    "- In this course, we prefer mergesort or heapsort for their worst case time complexity.\n",
    "- However, quicksort is widely used in practice, because the worst case is so rare and the constant factor is small.\n",
    "- Quicksort forms the basis of the default sort in many programming languages.\n",
    "\n",
    "### Efficient Comparison Sorts \n",
    "\n",
    "- We have seen three sorting algorithms which achieve O(n log n) time complexity for all or almost all inputs.\n",
    "- In this course, we are not concerned by the extra space used by merge sort, or the larger constant factor of heap sort.\n",
    "- However, unless explicitly directed otherwise, we always consider worst case performance, so quicksort’s O(n2) case is slow.\n",
    "- When designing algorithms in this course which use sorting by comparison, you can simply say ’sort the array using merge sort’ or ’using heapsort’.\n",
    "- However, the other algorithms are useful to understand conceptually and occasionally find some practical application.\n",
    "\n",
    "- Question: Is it possible to design a comparison sort which is asymptotically faster than merge sort in the worst case?\n",
    "- Answer: No!\n",
    "\n",
    "### Asymptotic lower bound on comparison sorting\n",
    "\n",
    "1. Claim: Any comparison sort must perform Ω(n log n) comparisons in the worst case.\n",
    "2. Proof: \n",
    "   1. There are n! permutations of the array.\n",
    "   2. In the worst case, only one of these is the correct sorted order. Our sorting algorithm must find which permutation this is.\n",
    "   1. An algorithm which performs k comparisons can get 2k different combinations of results from these comparisons, and therefore can distinguish between at most 2k permutations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
